## 1. **Отличие решаемых задач в теории вероятностей и математической статистики. Основная задача статистики. Вероятность. ЗБЧ. Описание случайных величин: дискретных и непрерывных. Характеристики распределений. Квантиль.**

**Теория вероятностей (ТВ) и математическая статистика (МС) решают обратные задачи**: ТВ предсказывает свойства выборок, зная закон распределения (параметры), а МС, наоборот, по выборке определяет этот закон и его параметры (мат. ожидание, дисперсию), оценивая достоверность. **Основная задача статистики** — анализ собранных данных для изучения закономерностей массовых явлений и получения выводов, используя методы ТВ для интерпретации случайности. 

**Вероятность события** — доля испытаний, завершившихся наступлением события, в бесконечном эксперименте.
**Закон больших чисел**: на больших выборках частота события хорошо приближает его вероятность.

**Дискретная случайная величина** $X$ принимает счётное множество значений $A=\{a_1, a_2, \ldots\}$  с вероятностями $p_1, p_2, \ldots$ и $\sum_i p_i = 1$.

$$f_X(a_i) = P(X = a_i) = p_i$$ - функция вероятности(плотности).

**Непрерывная случайная величина** задаётся с помощью функции распределения:

$$F_X(x) = P(X \leq x)$$

или плотности распределения:

$$ f_X(x) : \int_a^b f_X(x) \, dx = P(a \leq X \leq b) $$

**Характеристики распределений**:

Матожидание — среднее значение $X$:

$$ EX = \int x \, dF(x) $$

Дисперсия — мера разброса $X$:

$$\mathbb{D}X = \mathbb{E} \left( (X - EX)^2 \right)$$

Квантиль порядка $\alpha \in (0, 1)$:

$$X_\alpha : P(X \leq X_\alpha) \geq \alpha, \, P(X \geq X_\alpha) \geq 1 - \alpha$$

Эквивалентное определение:

$$X_\alpha = F^{-1} (\alpha) = \inf \{ x : F(x) \geq \alpha \}$$

Медиана — квантиль порядка 0.5, центральное значение распределения:

$$\text{med } X : P(X \leq \text{med } X) \geq 0.5, \, P(X \geq \text{med } X) \geq 0.5$$

Интерквартильный размах:

$$IQR = X_{0.75} - X_{0.25}$$

Мода — точка максимума функции вероятности или плотности:

$$\text{mode } X = \arg \max_x f(x)$$

Коэффициент ассиметрии (skewness):

$$\gamma_1 = \mathbb{E} \left( \frac{X - EX}{\sqrt{\mathbb{D}X}} \right)^3$$

![](Picture/img_1.png)

$\gamma_1 = 0$ — необходимое, но не достаточное условие симметричности:

![](Picture/img_2.png)

Коэффициент эксцесса (excess, без вычитания тройки — kurtosis):

$$ \gamma_2 = \frac{\mathbb{E}(X - \mathbb{E}X)^4}{(\mathbb{D}X)^2} - 3 $$

![](Picture/img_3.png)


## 2. **Распределение Бернулли. Биномиальное распределение. Распределение Пуассона. Распределения, производные от нормального: хи-квадрат, Стьюдента, Фишера. Определения, графики, свойства, примеры.**

### **Распределе́ние Берну́лли** — дискретное распределение вероятностей, моделирующее случайный эксперимент произвольной природы, при заранее известной вероятности успеха или неудачи. (Частный случай биномиального с n = 1)

$$X \in \{ 0,1 \} \sim Ber(p), p \in (0,1)$$

$$
\begin{align*}
F\left(x\right) & =\begin{cases}
0, & x<0,\\
1-p, & 0\leqslant x<1,\\
1, & x\geqslant 1.
\end{cases}\\
f\left(x\right) & =\begin{cases}
1-p, & x=0,\\
p, & x=1.
\end{cases}
\end{align*}
$$

![](Picture/img_4.png)

**Свойтсва:**

Предельное свойство описывается теоремой Пуассона:

Пусть есть последовательность серий испытаний Бернулли, где $p_n$ — вероятность «успеха», $\mu_n$ — количество «успехов».

Тогда если

1. $$\lim_{n \to \infty} p_n = 0; $$
2. $$\lim_{n \to \infty} np_n = \lambda; $$
3. $$\lambda > 0, $$

то $$\lim_{n \to \infty} P(\omega : \mu_n(\omega) = m) = e^{-\lambda} \frac{\lambda^m}{m!}. $$

**Моменты:**

$$E[X] = p.$$

$$D[X] = p(1 - p) = pq, \text{ так как } E X^2 - (E X)^2 = p - p^2 = p \cdot (1 - p) = pq.$$

Вообще, легко видеть, что

$$E[X^n] = Pr(X = 1) \cdot 1^n + Pr(X = 0) \cdot 0^n = p \cdot 1^n + q \cdot 0^n = p = E[X], \forall n \in \mathbb{N}$$

Пример: Подбрасывание монеты.

### **Биномиальное распределение** — дискретное распределение вероятностей случайной величины $X$, принимающей целочисленные значения $k=0,1,\ldots,n$ с вероятностями:  

$$P(x=k)=\binom{n}{k}p^{k}(1-p)^{n-k}.$$

Данное распределение характеризуется двумя параметрами: целым числом $n>0$, называемым числом испытаний, и вещественным числом $p,0\leq p\leq 1$, называемым вероятностью успеха в одном испытании.

$$X \in \{0,\ldots,N\} \sim Bin\left(N,p\right), N \in \mathbb{N}, p \in [0,1]$$

$$F\left(x\right) = I_{1-p}\left(N-x,1+x\right)$$

$$f\left(x\right) = C_N^x p^x \left(1-p\right)^{N-x}$$

![](Picture/img_5.png)

Свойства и моменты:

$$MX=np$$.  

$$DX=np(1-p)$$.

Пример: Стрельба по мишени

Стрелок попадает в цель с вероятностью $p = 0.8$. Он делает $n = 5$ выстрелов. Какова вероятность, что он попадет ровно 3 раза?

$$P(X = 3) = C_8^3 \cdot 0.8^3 \cdot 0.2^5 = 10 \cdot 0.512 \cdot 0.00032 = 0.2048 \quad (20.48\%).$$

### **Распределение Пуассона** - распределение числа независимых событий в фиксированном временном или пространственном интервале.

$$X \in \{0,1,2,\ldots\} \sim Pois(\lambda), \quad \lambda > 0$$

$$
\begin{align*}
&F(x) = e^{-\lambda} \sum_{i=0}^{\lfloor x \rfloor} \frac{\lambda^i}{i!} \\
&f(x) = e^{-\lambda} \frac{\lambda^x}{x!}
\end{align*}
$$

![](Picture/img_6.png)

Свойства и моменты:

$$\mathbb{E}X = \mathbb{D}X = \lambda$$

Пусть $X_1, \ldots, X_n$ независимы, $X_i \sim Pois(\lambda_i)$, тогда  
$$\sum_{i=1}^n X_i \sim Pois\left( \sum_{i=1}^n \lambda_i \right)$$

Eсли $X \sim Pois(\lambda)$, $Y = \sqrt{X}$, то при больших $\lambda$  
$$F_Y(x) \approx \Phi\left( \frac{x - \sqrt{\lambda}}{\sqrt{\lambda}} \right)$$

$Bin(n,p) \rightarrow_{n \to \infty} Pois(\lambda)$ при постоянном $np$  

Пример: количество изюма в булочке с изюмом

### **Распределение хи-квадрат** - eсли взять $k$ независимых случайных величин $Z_1, Z_2, \ldots, Z_k$, каждая из которых имеет **стандартное нормальное распределение** $N(0, 1)$, то сумма их квадратов будет иметь распределение хи-квадрат с $k$ степенями свободы:

$$X = \sum_{i=1}^k Z_i^2 \sim \chi^2(k)$$

$$X \in \mathbb{R}_+ \sim \chi_k^2, \quad k \in \mathbb{N}$$

$$F\left( x \right) = \frac{1}{\Gamma \left( \frac{k}{2} \right)} \gamma \left( \frac{k}{2}, \frac{x}{2} \right)$$

$$f\left( x \right) = \frac{1}{2^{\frac{k}{2}} \Gamma \left( \frac{k}{2} \right)} x^{\frac{k}{2} - 1} e^{-\frac{x}{2}}$$

$$\Gamma(a) = \int_0^\infty e^{-t} t^{x-1} dt \quad \text{— гамма-функция}$$

$$\gamma(a, x) = \int_0^x e^{-t} t^{a-1} dt \quad \text{— нижняя неполная гамма-функция}$$

![](Picture/img_7.png)

Свойтсва и моменты:

$$E[Y] = k$$

$$D[Y] = 2k$$

Пример: 

### Распределение Стьюдента

$$X \in \mathbb{R} \sim St(\nu), \quad \nu > 0$$

$$F(x) = \frac{1}{2} + x \Gamma\left(\frac{\nu+1}{2}\right)$$

$$f(x) = \frac{\Gamma\left(\frac{\nu+1}{2}\right)}{\sqrt{\nu\pi}\,\Gamma\left(\frac{\nu}{2}\right)} \left(1 + \frac{x^2}{\nu}\right)^{-\frac{\nu+1}{2}}$$

![](Picture/img_8.png)

Свойства и моменты:

$\mathbb{E}X = 0$ при $\nu > 1$, $\text{med}\,X = \text{mode}\,X = 0$ всегда

$D[X] = \frac{n}{n-2}$

Пусть $Z \sim N(0,1)$ и $V \sim \chi^2_{\nu}$ независимы, тогда

$$T = \frac{Z}{\sqrt{V/\nu}} \sim St(\nu)$$

если $X \sim St(\nu)$, то

$$Y = \lim_{\nu \to \infty} X \sim N(0,1)$$

Пример: возникает при оценке среднего значения случайной величины с неизвестной дисперсией

### Распределение Фишера

Пусть $X_1 \sim \chi_{d_1}^2$, $X_2 \sim \chi_{d_2}^2$, $X_1$ и $X_2$ независимы, тогда  

$$\frac{X_1 / d_1}{X_2 / d_2} \sim F(d_1, d_2)$$

Если $X \sim F(d_1, d_2)$, то  

$$Y = \lim_{d_2 \to \infty} d_1 X \sim \chi_{d_1}^2$$

$$F(x; d_1, d_2) = F(1/x; d_2, d_1)$$

$$X \in \mathbb{R}_+ \sim F(d_1, d_2), \quad d_1, d_2 > 0$$

$$F(x) = I_{\frac{d_1 x}{d_1 x + d_2}} \left( \frac{d_1}{2}, \frac{d_2}{2} \right)$$

$$f(x) = \sqrt{\frac{(d_1 x)^{d_1} d_2^{d_2}}{(d_1 x + d_2)^{d_1 + d_2}}} \left( x B \left( \frac{d_1}{2}, \frac{d_2}{2} \right) \right)$$

$$B(a, b) = \int_0^1 t^{a-1} (1-t)^{b-1} dt \quad \text{— бета-функция}$$

$$I_x(a, b) = \frac{B(x; a, b)}{B(a, b)} \quad \text{— регуляризованная неполная бета-функция}$$

$$B(x; a, b) = \int_0^x t^{a-1} (1-t)^{b-1} dt \quad \text{— неполная бета-функция}$$

![](Picture/img_9.png)

Свойства и моменты:

Его форма определяется двумя параметрами — степенями свободы числителя и знаменателя.

$$E[X] = \frac{d_2}{d_2 - 2}, \quad \text{при } d_2 > 2$$

**Важное условие:** Если $d_2 \leq 2$, математическое ожидание не определено (бесконечно).

$$\text{Var}(X) = \frac{2d_2^2(d_1 + d_2 - 2)}{d_1(d_2 - 2)^2(d_2 - 4)}, \quad \text{при } d_2 > 4$$

**Важное условие:** Дисперсия существует только в том случае, если число степеней свободы знаменателя строго больше $4$. Если $d_2 \leq 4$, дисперсия не определена.

Пример: возникает в дисперсионном и регрессионном анализе

## 3.Нормальное распределение: стандартное и с параметрами μ, σ. Приведение к стандартному виду. Определения, графики, свойства, примеры. Правило трёх сигм. Теорема Крамера. ЦПТ.

$$X \in \mathbb{R} \sim N\left(\mu, \sigma^2\right), \quad \sigma^2 > 0$$

$$F(x) = \Phi \left( \frac{x - \mu}{\sigma} \right)$$

$$f(x) = \frac{1}{\sigma} \phi \left( \frac{x - \mu}{\sigma} \right)$$

$$\Phi (x) = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{x} e^{-\frac{t^2}{2}} dt$$

$$\phi (x) = \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}}$$

Стандартное нормальное распределение - Это частный случай, когда $\mu = 0$, а $\sigma = 1$. Обозначается как $Z \sim N(0, 1)$. Его плотность упрощается до:

$$\phi(z) = \frac{1}{\sqrt{2\pi}} e^{-\frac{z^2}{2}}$$

![](Picture/img_10.png)

Свойства и моменты:

**предельное распределение суммы слабо взаимозависимых случайных величин**

$\mathbb{E} X = \text{med } X = \text{mode } X = \mu$, $\mathbb{D} X = \sigma^2$, все моменты более высокого порядка нулевые

**пусть** $X_1, \ldots, X_n$ **независимы,** $X_i \sim N\left(\mu_i, \sigma_i^2\right)$, **тогда** $\forall a_1, \ldots, a_n$  

$$\sum_{i=1}^n a_i X_i \sim N \left( \sum_{i=1}^n a_i \mu_i, \sum_{i=1}^n a_i^2 \sigma_i^2 \right)$$

Пример: погрешность измерения

### Приведение к стандартному виду (Z-преобразование)

Любую нормальную величину $X$ можно превратить в стандартную $Z$. Это нужно для использования статистических таблиц:

$$Z = \frac{X - \mu}{\sigma}$$

Это действие называют **центрированием** (вычитаем $\mu$) и **нормированием** (делим на $\sigma$).

### **Правило трёх сигм** — это эмпирическое правило, которое позволяет быстро оценить разброс данных в нормальном распределении. Оно гласит, что практически все значения (99.7%) нормально распределённой случайной величины лежат в интервале $\pm 3\sigma$ от её среднего значения $\mu$.

### Суть правила

Для нормального распределения вероятность того, что случайная величина отклонится от своего математического ожидания, распределяется следующим образом:

1. **$1\sigma$ (Правило 68%):** Около 68.27% данных лежат в пределах одного стандартного отклонения от среднего $[\mu - \sigma; \mu + \sigma]$.  
2. **$2\sigma$ (Правило 95%):** Около 95.45% данных лежат в пределах двух стандартных отклонений от среднего $[\mu - 2\sigma; \mu + 2\sigma]$.  
3. **$3\sigma$ (Правило 99.7%):** Около 99.73% данных лежат в пределах трёх стандартных отклонений от среднего $[\mu - 3\sigma; \mu + 3\sigma]$.

Это правило — главный инструмент для поиска аномалий (выбросов). Если какое-то значение выходит за пределы трёх сигм, оно считается крайне маловероятным (вероятность всего 0.27%) и, скорее всего, вызвано какой-то ошибкой или экстраординарным фактором.

### Теорема Крамера

Пусть случайная величина  имеет нормальное распределение и представима в виде суммы двух независимых случайных величин ξ=1+2. Тогда 1и 2 также нормально распределены.

### ЦПТ

**центральная предельная теорема:** пусть $X_1, \ldots, X_n$ **i.i.d. с $\mathbb{E}X$ и $\mathbb{D}X < \infty$, тогда**

$$\frac{1}{n} \sum_{i=1}^n X_i \sim N \left( \mathbb{E} X, \frac{\mathbb{D} X}{n} \right)$$

Если вы возьмете большое количество независимых случайных величин с любым распределением (даже очень странным), их сумма или среднее значение будет стремиться к нормальному распределению при увеличении числа слагаемых.
