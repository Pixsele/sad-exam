## 1. **Отличие решаемых задач в теории вероятностей и математической статистики. Основная задача статистики. Вероятность. ЗБЧ. Описание случайных величин: дискретных и непрерывных. Характеристики распределений. Квантиль.**

**Теория вероятностей (ТВ) и математическая статистика (МС) решают обратные задачи**: ТВ предсказывает свойства выборок, зная закон распределения (параметры), а МС, наоборот, по выборке определяет этот закон и его параметры (мат. ожидание, дисперсию), оценивая достоверность. **Основная задача статистики** — анализ собранных данных для изучения закономерностей массовых явлений и получения выводов, используя методы ТВ для интерпретации случайности. 

**Вероятность события** — доля испытаний, завершившихся наступлением события, в бесконечном эксперименте.
**Закон больших чисел**: на больших выборках частота события хорошо приближает его вероятность.

**Дискретная случайная величина** $X$ принимает счётное множество значений $A=\{a_1, a_2, \ldots\}$  с вероятностями $p_1, p_2, \ldots$ и $\sum_i p_i = 1$.

$$f_X(a_i) = P(X = a_i) = p_i$$ - функция вероятности(плотности).

**Непрерывная случайная величина** задаётся с помощью функции распределения:

$$F_X(x) = P(X \leq x)$$

или плотности распределения:

$$ f_X(x) : \int_a^b f_X(x) \, dx = P(a \leq X \leq b) $$

**Характеристики распределений**:

Матожидание — среднее значение $X$:

$$ EX = \int x \, dF(x) $$

Дисперсия — мера разброса $X$:

$$\mathbb{D}X = \mathbb{E} \left( (X - EX)^2 \right)$$

Квантиль порядка $\alpha \in (0, 1)$:

$$X_\alpha : P(X \leq X_\alpha) \geq \alpha, \, P(X \geq X_\alpha) \geq 1 - \alpha$$

Эквивалентное определение:

$$X_\alpha = F^{-1} (\alpha) = \inf \{ x : F(x) \geq \alpha \}$$

Медиана — квантиль порядка 0.5, центральное значение распределения:

$$\text{med } X : P(X \leq \text{med } X) \geq 0.5, \, P(X \geq \text{med } X) \geq 0.5$$

Интерквартильный размах:

$$IQR = X_{0.75} - X_{0.25}$$

Мода — точка максимума функции вероятности или плотности:

$$\text{mode } X = \arg \max_x f(x)$$

Коэффициент ассиметрии (skewness):

$$\gamma_1 = \mathbb{E} \left( \frac{X - EX}{\sqrt{\mathbb{D}X}} \right)^3$$

![](Picture/img_1.png)

$\gamma_1 = 0$ — необходимое, но не достаточное условие симметричности:

![](Picture/img_2.png)

Коэффициент эксцесса (excess, без вычитания тройки — kurtosis):

$$ \gamma_2 = \frac{\mathbb{E}(X - \mathbb{E}X)^4}{(\mathbb{D}X)^2} - 3 $$

![](Picture/img_3.png)

---

## 2. **Распределение Бернулли. Биномиальное распределение. Распределение Пуассона. Распределения, производные от нормального: хи-квадрат, Стьюдента, Фишера. Определения, графики, свойства, примеры.**

### **Распределе́ние Берну́лли** — дискретное распределение вероятностей, моделирующее случайный эксперимент произвольной природы, при заранее известной вероятности успеха или неудачи. (Частный случай биномиального с n = 1)

$$X \in \{ 0,1 \} \sim Ber(p), p \in (0,1)$$

$$
\begin{align*}
F\left(x\right) & =\begin{cases}
0, & x<0,\\
1-p, & 0\leqslant x<1,\\
1, & x\geqslant 1.
\end{cases}\\
f\left(x\right) & =\begin{cases}
1-p, & x=0,\\
p, & x=1.
\end{cases}
\end{align*}
$$

![](Picture/img_4.png)

**Свойтсва:**

Предельное свойство описывается теоремой Пуассона:

Пусть есть последовательность серий испытаний Бернулли, где $p_n$ — вероятность «успеха», $\mu_n$ — количество «успехов».

Тогда если

1. $$\lim_{n \to \infty} p_n = 0; $$
2. $$\lim_{n \to \infty} np_n = \lambda; $$
3. $$\lambda > 0, $$

то $$\lim_{n \to \infty} P(\omega : \mu_n(\omega) = m) = e^{-\lambda} \frac{\lambda^m}{m!}. $$

**Моменты:**

$$E[X] = p.$$

$$D[X] = p(1 - p) = pq, \text{ так как } E X^2 - (E X)^2 = p - p^2 = p \cdot (1 - p) = pq.$$

Вообще, легко видеть, что

$$E[X^n] = Pr(X = 1) \cdot 1^n + Pr(X = 0) \cdot 0^n = p \cdot 1^n + q \cdot 0^n = p = E[X], \forall n \in \mathbb{N}$$

Пример: Подбрасывание монеты.

### **Биномиальное распределение** — дискретное распределение вероятностей случайной величины $X$, принимающей целочисленные значения $k=0,1,\ldots,n$ с вероятностями:  

$$P(x=k)=\binom{n}{k}p^{k}(1-p)^{n-k}.$$

Данное распределение характеризуется двумя параметрами: целым числом $n>0$, называемым числом испытаний, и вещественным числом $p,0\leq p\leq 1$, называемым вероятностью успеха в одном испытании.

$$X \in \{0,\ldots,N\} \sim Bin\left(N,p\right), N \in \mathbb{N}, p \in [0,1]$$

$$F\left(x\right) = I_{1-p}\left(N-x,1+x\right)$$

$$f\left(x\right) = C_N^x p^x \left(1-p\right)^{N-x}$$

![](Picture/img_5.png)

Свойства и моменты:

$$MX=np$$.  

$$DX=np(1-p)$$.

Пример: Стрельба по мишени

Стрелок попадает в цель с вероятностью $p = 0.8$. Он делает $n = 5$ выстрелов. Какова вероятность, что он попадет ровно 3 раза?

$$P(X = 3) = C_8^3 \cdot 0.8^3 \cdot 0.2^5 = 10 \cdot 0.512 \cdot 0.00032 = 0.2048 \quad (20.48\%).$$

### **Распределение Пуассона** - распределение числа независимых событий в фиксированном временном или пространственном интервале.

$$X \in \{0,1,2,\ldots\} \sim Pois(\lambda), \quad \lambda > 0$$

$$
\begin{align*}
&F(x) = e^{-\lambda} \sum_{i=0}^{\lfloor x \rfloor} \frac{\lambda^i}{i!} \\
&f(x) = e^{-\lambda} \frac{\lambda^x}{x!}
\end{align*}
$$

![](Picture/img_6.png)

Свойства и моменты:

$$\mathbb{E}X = \mathbb{D}X = \lambda$$

Пусть $X_1, \ldots, X_n$ независимы, $X_i \sim Pois(\lambda_i)$, тогда  
$$\sum_{i=1}^n X_i \sim Pois\left( \sum_{i=1}^n \lambda_i \right)$$

Eсли $X \sim Pois(\lambda)$, $Y = \sqrt{X}$, то при больших $\lambda$  
$$F_Y(x) \approx \Phi\left( \frac{x - \sqrt{\lambda}}{\sqrt{\lambda}} \right)$$

$Bin(n,p) \rightarrow_{n \to \infty} Pois(\lambda)$ при постоянном $np$  

Пример: количество изюма в булочке с изюмом

### **Распределение хи-квадрат** - eсли взять $k$ независимых случайных величин $Z_1, Z_2, \ldots, Z_k$, каждая из которых имеет **стандартное нормальное распределение** $N(0, 1)$, то сумма их квадратов будет иметь распределение хи-квадрат с $k$ степенями свободы:

$$X = \sum_{i=1}^k Z_i^2 \sim \chi^2(k)$$

$$X \in \mathbb{R}_+ \sim \chi_k^2, \quad k \in \mathbb{N}$$

$$F\left( x \right) = \frac{1}{\Gamma \left( \frac{k}{2} \right)} \gamma \left( \frac{k}{2}, \frac{x}{2} \right)$$

$$f\left( x \right) = \frac{1}{2^{\frac{k}{2}} \Gamma \left( \frac{k}{2} \right)} x^{\frac{k}{2} - 1} e^{-\frac{x}{2}}$$

$$\Gamma(a) = \int_0^\infty e^{-t} t^{a-1} dt \quad \text{— гамма-функция}$$

$$\gamma(a, x) = \int_0^x e^{-t} t^{a-1} dt \quad \text{— нижняя неполная гамма-функция}$$

![](Picture/img_7.png)

Свойтсва и моменты:

$$E[Y] = k$$

$$D[Y] = 2k$$

Пример: 

### Распределение Стьюдента

$$X \in \mathbb{R} \sim St(\nu), \quad \nu > 0$$

$$F(x) = \frac{1}{2} + x \Gamma\left(\frac{\nu+1}{2}\right)$$

$$f(x) = \frac{\Gamma\left(\frac{\nu+1}{2}\right)}{\sqrt{\nu\pi}\,\Gamma\left(\frac{\nu}{2}\right)} \left(1 + \frac{x^2}{\nu}\right)^{-\frac{\nu+1}{2}}$$

![](Picture/img_8.png)

Свойства и моменты:

$\mathbb{E}X = 0$ при $\nu > 1$, $\text{med}\,X = \text{mode}\,X = 0$ всегда

$D[X] = \frac{n}{n-2}$

Пусть $Z \sim N(0,1)$ и $V \sim \chi^2_{\nu}$ независимы, тогда

$$T = \frac{Z}{\sqrt{V/\nu}} \sim St(\nu)$$

если $X \sim St(\nu)$, то

$$Y = \lim_{\nu \to \infty} X \sim N(0,1)$$

Пример: возникает при оценке среднего значения случайной величины с неизвестной дисперсией

### Распределение Фишера

Пусть $X_1 \sim \chi_{d_1}^2$, $X_2 \sim \chi_{d_2}^2$, $X_1$ и $X_2$ независимы, тогда  

$$\frac{X_1 / d_1}{X_2 / d_2} \sim F(d_1, d_2)$$

Если $X \sim F(d_1, d_2)$, то  

$$Y = \lim_{d_2 \to \infty} d_1 X \sim \chi_{d_1}^2$$

$$F(x; d_1, d_2) = F(1/x; d_2, d_1)$$

$$X \in \mathbb{R}_+ \sim F(d_1, d_2), \quad d_1, d_2 > 0$$

$$F(x) = I_{\frac{d_1 x}{d_1 x + d_2}} \left( \frac{d_1}{2}, \frac{d_2}{2} \right)$$

$$f(x) = \sqrt{\frac{(d_1 x)^{d_1} d_2^{d_2}}{(d_1 x + d_2)^{d_1 + d_2}}} \left( x B \left( \frac{d_1}{2}, \frac{d_2}{2} \right) \right)$$

$$B(a, b) = \int_0^1 t^{a-1} (1-t)^{b-1} dt \quad \text{— бета-функция}$$

$$I_x(a, b) = \frac{B(x; a, b)}{B(a, b)} \quad \text{— регуляризованная неполная бета-функция}$$

$$B(x; a, b) = \int_0^x t^{a-1} (1-t)^{b-1} dt \quad \text{— неполная бета-функция}$$

![](Picture/img_9.png)

Свойства и моменты:

Его форма определяется двумя параметрами — степенями свободы числителя и знаменателя.

$$E[X] = \frac{d_2}{d_2 - 2}, \quad \text{при } d_2 > 2$$

**Важное условие:** Если $d_2 \leq 2$, математическое ожидание не определено (бесконечно).

$$\text{Var}(X) = \frac{2d_2^2(d_1 + d_2 - 2)}{d_1(d_2 - 2)^2(d_2 - 4)}, \quad \text{при } d_2 > 4$$

**Важное условие:** Дисперсия существует только в том случае, если число степеней свободы знаменателя строго больше $4$. Если $d_2 \leq 4$, дисперсия не определена.

Пример: возникает в дисперсионном и регрессионном анализе

---

## 3.Нормальное распределение: стандартное и с параметрами μ, σ. Приведение к стандартному виду. Определения, графики, свойства, примеры. Правило трёх сигм. Теорема Крамера. ЦПТ.

$$X \in \mathbb{R} \sim N\left(\mu, \sigma^2\right), \quad \sigma^2 > 0$$

$$F(x) = \Phi \left( \frac{x - \mu}{\sigma} \right)$$

$$f(x) = \frac{1}{\sigma} \phi \left( \frac{x - \mu}{\sigma} \right)$$

$$\Phi (x) = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{x} e^{-\frac{t^2}{2}} dt$$

$$\phi (x) = \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}}$$

Стандартное нормальное распределение - Это частный случай, когда $\mu = 0$, а $\sigma = 1$. Обозначается как $Z \sim N(0, 1)$. Его плотность упрощается до:

$$\phi(z) = \frac{1}{\sqrt{2\pi}} e^{-\frac{z^2}{2}}$$

![](Picture/img_10.png)

Свойства и моменты:

**предельное распределение суммы слабо взаимозависимых случайных величин**

$\mathbb{E} X = \text{med } X = \text{mode } X = \mu$, $\mathbb{D} X = \sigma^2$, все моменты более высокого порядка нулевые

**Сумма нормальных распределений(или линейная комбинация):**

**пусть** $X_1, \ldots, X_n$ **независимы,** $X_i \sim N\left(\mu_i, \sigma_i^2\right)$, **тогда** $\forall a_1, \ldots, a_n$  

$$\sum_{i=1}^n a_i X_i \sim N \left( \sum_{i=1}^n a_i \mu_i, \sum_{i=1}^n a_i^2 \sigma_i^2 \right)$$

Пример: погрешность измерения

### Приведение к стандартному виду (Z-преобразование)

Любую нормальную величину $X$ можно превратить в стандартную $Z$. Это нужно для использования статистических таблиц:

$$Z = \frac{X - \mu}{\sigma}$$

Это действие называют **центрированием** (вычитаем $\mu$) и **нормированием** (делим на $\sigma$).

### **Правило трёх сигм** — это эмпирическое правило, которое позволяет быстро оценить разброс данных в нормальном распределении. Оно гласит, что практически все значения (99.7%) нормально распределённой случайной величины лежат в интервале $\pm 3\sigma$ от её среднего значения $\mu$.

### Суть правила

Для нормального распределения вероятность того, что случайная величина отклонится от своего математического ожидания, распределяется следующим образом:

1. **$1\sigma$ (Правило 68%):** Около 68.27% данных лежат в пределах одного стандартного отклонения от среднего $[\mu - \sigma; \mu + \sigma]$.  
2. **$2\sigma$ (Правило 95%):** Около 95.45% данных лежат в пределах двух стандартных отклонений от среднего $[\mu - 2\sigma; \mu + 2\sigma]$.  
3. **$3\sigma$ (Правило 99.7%):** Около 99.73% данных лежат в пределах трёх стандартных отклонений от среднего $[\mu - 3\sigma; \mu + 3\sigma]$.

Это правило — главный инструмент для поиска аномалий (выбросов). Если какое-то значение выходит за пределы трёх сигм, оно считается крайне маловероятным (вероятность всего 0.27%) и, скорее всего, вызвано какой-то ошибкой или экстраординарным фактором.

### Теорема Крамера

Пусть случайная величина $\xi$ имеет нормальное распределение и представима в виде суммы двух независимых случайных величин  
$$\xi = \xi_1 + \xi_2.$$
Тогда $\xi_1$ и $\xi_2$ также нормально распределены.

### ЦПТ

**центральная предельная теорема:** пусть $X_1, \ldots, X_n$ **i.i.d. с $\mathbb{E}X$ и $\mathbb{D}X < \infty$, тогда**

$$\frac{1}{n} \sum_{i=1}^n X_i \sim N \left( \mathbb{E} X, \frac{\mathbb{D} X}{n} \right)$$

Если вы возьмете большое количество независимых случайных величин с любым распределением (даже очень странным), их сумма или среднее значение будет стремиться к нормальному распределению при увеличении числа слагаемых.

---

## 4.Генеральная совокупность. Выборка. Эмпирическая функция распределения. Теорема Гливенко — Кантелли. Гистограмма частот. Статистика (функция выборки). Статистики, используемые для оценки характеристик распределений. выборочные моменты, порядковая статистика, интерквартильный размах. Боксплот (ящик с усами).

**Генеральная совокупность** — множество объектов, свойства которых подлежат изучению в рассматриваемой задаче.

**Выборка** — конечное множество объектов, отобранных из генеральной совокупности для проведения измерений.

$$X^n = (X_1, \ldots, X_n).$$

$n$ — объём выборки.

$X^n$ — **простая выборка**, если $X_1, \ldots, X_n$ — независимые одинаково распределённые случайные величины (i.i.d.).

Основная задача статистики — описание $F_X(x)$ по реализации выборки.

**Функция распределения**

$$F_n(x) = \frac{1}{n} \sum_{i=1}^n \mathbb{I}[X_i \leq x]$$ — **эмпирическая функция распределения.**

![](Picture/img_11.png)

![](Picture/img_12.png)

**Теорема Гливе́нко — Канте́лли** в математической статистике уточняет результат о сходимости выборочной функции распределения к её теоретическому аналогу.

Пусть $X_1, \ldots, X_n, \ldots$ — бесконечная выборка из распределения, задаваемого функцией распределения $F$.

Пусть $\hat{F}$ — выборочная функция распределения, построенная на первых $n$ элементах выборки. Тогда

$$\lim_{n \to \infty} \sup_{x \in \mathbb{R}} |\hat{F}(x) - F(x)| = 0$$

почти наверное,

где символ $\sup$ обозначает точную верхнюю грань.

Гистограмма частот — это ступенчатый график в виде прямоугольников, показывающий распределение числовых данных: ось X делит данные на интервалы (классы), а высота каждого столбика отражает частоту (количество) значений, попавших в этот интервал.

![](Picture/img_13.png)

**Статистика $T(X^n)$** — любая измеримая функция выборки. Например:

Выборочное среднее: 

$$\bar{X} = \frac{1}{n} \sum_{i=1}^{n} X_i$$

Выборочная дисперсия: 

$$S^2 = \frac{1}{n-1} \sum_{i=1}^{n} (X_i - \bar{X})^2$$

Вариационный ряд - $X_{(1)} \leq X_{(2)} \leq \ldots \leq X_{(n)}$ , где

ранг элемента выборки - $X_i$: $\text{rank}(X_i) = r \quad \text{такой, что } X_i = X_{(r)}$

$k$-я порядковая статистика: $X_{(k)}$

Выборочный $\alpha$-квантиль: $X_{(\lfloor \alpha n \rfloor)}$

Выборочная медиана:  

  $$m = 
  \begin{cases} 
    X_{(k+1)}, & \text{если } n = 2k + 1, \\
    \frac{X_{(k)} + X_{(k+1)}}{2}, & \text{если } n = 2k.
  \end{cases}$$

Выборочный интерквартильный размах: 

$$IQR_n = X_{(\lfloor 0.75n \rfloor)} - X_{(\lfloor 0.25n \rfloor)}$$

Выборочный коэффициент асимметрии: 

$$g_1 = \sqrt{n} \frac{\sum_{i=1}^{n} (X_i - \bar{X})^3}{\left( \sum_{i=1}^{n} (X_i - \bar{X})^2 \right)^{3/2}}$$

Выборочный коэффициент эксцесса: 

$$g_2 = \frac{n \sum_{i=1}^{n} (X_i - \bar{X})^4}{\left( \sum_{i=1}^{n} (X_i - \bar{X})^2 \right)^2} - 3$$

Боксплот (или ящик с усами) — это графическое представление, используемое для визуализации распределения данных через показатели центра (медиана), разброса (квартили) и наличие выбросов. Внешне боксплот представляет собой прямоугольник (ящик), высота которого показывает интерквартильный размах (разницу между верхним и нижним квартилями), внутри которого находится линия, отображающая медиану. От ящика выходят "усы", которые простираются до минимального и максимального наблюдаемого значения, за исключением выбросов.

![](Picture/img_14.png)

---

## 5.Точечные оценки параметров распределения, свойства. Метод моментов. Метод максимального правдоподобия, свойства.

Пусть распределение генеральной совокупности параметрическое:

$$F(x) = F(x, \theta).$$

Статистика $\hat{\theta}_n = \hat{\theta}(X^n)$ — **точечная оценка параметра $\theta$.**

Какая оценка лучше?

**Состоятельность:**

$$\lim_{n \to \infty} \hat{\theta}_n = \theta.$$

**Несмещённость:**

$$\mathbb{E}\hat{\theta}_n = \theta.$$

**Асимптотическая несмещённость:**

$$\lim_{n \to \infty} \mathbb{E}\hat{\theta}_n = \theta.$$

**Оптимальность:**

$$\mathbb{D}\hat{\theta}_n = \min_{\hat{\theta}_1 \in \mathcal{U}} \mathbb{D}\hat{\theta}_1,$$

где $\mathcal{U}$ — класс несмещённых оценок.

**Робастность:**

устойчивость $\hat{\theta}_n$ относительно

- отклонений истинного распределения $X$ от модельного семейства

- выбросов, содержащихся в выборке

### Метод моментов

**Оценка одного параметра.** Пусть известен вид плотности распределения $f(x, \theta)$, зависящей от одного параметра $\theta$, но не известно значение этого параметра.

Для нахождения его оценки достаточно составить одно уравнение, например, для начальных моментов первого порядка:  
$$\bar{x}_B = M(X).$$

Так как математическое ожидание признака генеральной совокупности

$$M(X) = \int_{-\infty}^{\infty} x \cdot f(x, \theta) dx = \varphi(\theta)$$

зависит от неизвестного параметра $\theta$, а выборочное среднее

$$\bar{x}_B = \frac{x_1 + x_2 + \cdots + x_n}{n}$$

зависит от реализации выборки

$$x_1, x_2, \ldots, x_n,$$

то после решения уравнения

$$\varphi(\theta) = \frac{x_1 + x_2 + \cdots + x_n}{n}$$

мы получаем

$$\theta^* = \psi(x_1, x_2, \ldots, x_n)$$

— оценку неизвестного параметра как функцию значений конкретной выборки.

### Метод максимального правдоподобия

**Дискретные случайные величины.** Пусть $X$ – дискретная случайная величина, для которой в результате опыта получена выборка значений $x_1, x_2, \ldots, x_n$. Вид закона распределения известен; закон содержит неизвестный параметр $\theta$, для которого требуется найти точечную оценку на основании данных выборки.

Обозначим вероятность $P(X = x_i) = p(x_i; \theta),\; i = 1, 2, \ldots, n$.

**Функцией правдоподобия** дискретной случайной величины $X$ называют функцию аргумента $\theta$ и данной выборки $x_1, x_2, \ldots, x_n$:

$$L(x_1, x_2, \ldots, x_n; \theta) = p(x_1; \theta) \cdot p(x_2; \theta) \cdot \ldots \cdot p(x_n; \theta).$$

В качестве точечной оценки параметра $\theta$ принимается значение $\theta^* = \theta^*(x_1, x_2, \ldots, x_n)$, при котором функция правдоподобия достигает наибольшего значения. Полученную оценку называют **оценкой максимального правдоподобия**.

Так как функции $L$ и $\ln L$ достигают максимума при одном и том же значении $\theta$, при практических вычислениях чаще используют вторую функцию, называемую **логарифмической функцией правдоподобия**:

$$\ln L(x_1, x_2, \ldots, x_n; \theta) = \ln p(x_1; \theta) + \ln p(x_2; \theta) + \ldots + \ln p(x_n; \theta).$$

Свойства:

- **состоятельность:**

$$\lim_{n \to \infty} \hat{\theta}_{MLE} = \theta$$

- **асимптотическая нормальность:** при $n \to \infty$

$$\hat{\theta}_{MLE} \sim N \left( \theta, I^{-1}(\theta) \right)$$

- **эффективность:** ОМП имеют наименьшую дисперсию среди всех состоятельных оценок  
- **инвариантность:**

$$g \left( \hat{\theta}_{MLE} \right) \text{ — ОМП-оценка для } g(\theta)$$

---

## 6.Квантили и построение доверительных интервалов. Интервальные оценки с помощью квантилей: для нормального распределения, для ненормальных распределений. Отличие z-доверительного интревала. Бутстреп-распределение, базовый бутстреп, свойства.

Квантили — это значения, разделяющие упорядоченный набор данных на равные доли. Например, медиана является вторым квартилем (т.е. 50%-квантилью).

Квантиль порядка $\alpha \in (0, 1)$:

$$X_\alpha : P(X \leq X_\alpha) \geq \alpha, \, P(X \geq X_\alpha) \geq 1 - \alpha$$

Интервальная оценка неизвестного параметра $\Theta$ – случайные функции $\Theta_1^* (X_1, X_2, \ldots, X_n)$ и $\Theta_2^* (X_1, X_2, \ldots, X_n)$ такие, что

$$P (\Theta_1^* < \Theta < \Theta_2^*) = \gamma = 1 - \alpha,$$  

т.е. интервал $(\Theta_1^*, \Theta_2^*)$ заключает в себе (покрывает) неизвестный параметр $\Theta$ с вероятностью $\gamma$.  

Сам интервал носит название **доверительного интервала**, величина $\gamma = 1 - \alpha$ называется **доверительной вероятностью** оценки (надёжностью, коэффициентом доверия), числа $\Theta_1^*$ и $\Theta_2^*$ – **доверительными границами**, $\alpha$ – **уровнем значимости**.

Непараметрический доверительный интервал для медианы непрерывного распределения.

$$X^n = (X_1, \ldots, X_n), \quad X \sim F(x) \Rightarrow$$

$$\mathbf{P}(X_{(r)} \leq \text{med}\,X) = \sum_{i=r+1}^n C_n^i \frac{1}{2^i} \cdot \frac{1}{2^{n-i}},$$

$$\mathbf{P}(\text{med}\,X \in [X_{(r)}, X_{(n-r+1)}]) = \frac{1}{2^n} \sum_{i=r}^{n-r+1} C_n^i.$$

При $n > 10$ применима нормальная аппроксимация:

$$\mathbf{P}\left(\text{med}\,X \in \left[ X_{\left(\frac{n-\sqrt{n}z_{1-\alpha/2}}{2}\right)} ,\; X_{\left(\frac{n+\sqrt{n}z_{1-\alpha/2}}{2}\right)} \right] \right) \approx 1 - \alpha.$$

Аналогично строится непараметрический доверительный интервал для любого квантиля $X_\alpha, \alpha \in (0, 1)$:

$$\mathbf{P}(X_\alpha \in [X_{(l)}, X_{(u)}]) = \sum_{i=l}^u C_n^i \alpha^i (1 - \alpha)^{n-i}.$$

**Параметрический метод**

Параметрический метод используется, когда предполагается известная форма распределения данных и известные параметры этого распределения. Например, если данные приближаются нормальным распределением, то используются формулы для построения доверительных интервалов на основе стандартного нормального распределения или распределения Стьюдента в зависимости от известности или неизвестности дисперсии.

**Пример:**

**Доверительный интервал для среднего с известной дисперсией:**

$$\left[ \bar{X} - z_{\alpha/2} \frac{\sigma}{\sqrt{n}},\; \bar{X} + z_{\alpha/2} \frac{\sigma}{\sqrt{n}} \right]$$

где $\bar{X}$ — выборочное среднее, $\sigma$ — известное стандартное отклонение, $z_{\alpha/2}$ — квантиль стандартного нормального распределения.

**Наивный метод**

Наивный метод _(или метод “по-быстрому”)_ используется без предположений о распределении данных или их параметрах. Он основывается на статистической интуиции и может быть применён в ситуациях, когда нет точной информации о распределении или мало данных для более сложных методов.

**Пример:**

Доверительный интервал для доли:  

$$\left[ \hat{p} - z_{\alpha/2} \sqrt{\frac{\hat{p}(1 - \hat{p})}{n}},\; \hat{p} + z_{\alpha/2} \sqrt{\frac{\hat{p}(1 - \hat{p})}{n}} \right]$$

где $\hat{p}$ — выборочная доля, $z_{\alpha/2}$ — квантиль стандартного нормального распределения.

